## 映射参数

[toc]



### fields

可以让同一字段有多种不同的索引方式, 例如:

1. 创建索引

   ```json
   PUT blog
   {   
       "settings": {
           "number_of_replicas": 0,
           "index.mapping.coerce": false
       },
       "mappings": {
           "_doc": {
               "properties": {
                   "title": {
                       "type": "text",
                       "fields": {
                           "raw": {
                               "type": "keyword"
                           }
                       }
                   }
               }
           }   
       }
   }
   ```

2. 添加文档: 

   ```json
   POST blog/_doc/1
   {
      "title": "zhang san"
   }
   ```

3. 搜索文档:

   通过term用全部关键词查询会查不到

   ```json
   GET/POST blog/_doc/_search
   {
       "query": {
           "term": {
               "title": "zhang san"
           }
       }
   }
   ```

   查询 title.raw 就可以了

   ```json
   GET/POST blog/_doc/_search
   {
       "query": {
           "term": {
               "title.raw": "zhang san"
           }
       }
   }
   ```

<br>

### analyzer

定义文本字段的分词器(索引数据时的分词器), 在添加文档和查询文档时都是有效的(索引数据和查询数据时, 用的是同一个分词器)

我们使用默认的分词器, 我们先看下添加文档后, 分词的效果:

1. 创建索引

   ~~~json
   PUT blog
   {   
       "settings": {
           "number_of_replicas": 0
       },
       "mappings": {
           "_doc": {
               "properties": {
                   "title": {
                       "type": "text"
                   }
               }
           }   
       }
   }
   ~~~

2. 添加文档

   ~~~json
   PUT blog/_doc/1
   {
      "title": "zhan san"
   }
   ~~~

3. 查看词条向量语句

   ```json
   GET blog/_doc/1/_termvectors
   {
       "fields": ["title"]
   }
   ```

4. 查看分词结果

   ```json
   
   {
       "_index": "blog",
       "_type": "_doc",
       "_id": "1",
       "_version": 1,
       "found": true,
       "took": 97,
       "term_vectors": {
           "title": {
               "field_statistics": {
                   "sum_doc_freq": 2,
                   "doc_count": 1,
                   "sum_ttf": 2
               },
               "terms": {
                   "san": {
                       "term_freq": 1,
                       "tokens": [
                           {
                               "position": 1,
                               "start_offset": 5,
                               "end_offset": 8
                           }
                       ]
                   },
                   "zhan": {
                       "term_freq": 1,
                       "tokens": [
                           {
                               "position": 0,
                               "start_offset": 0,
                               "end_offset": 4
                           }
                       ]
                   }
               }
           }
       }
   }
   ```

查询结果如下: 使用 term 精准查询文档: 

term查询不到结果, match可以, (match 是全文查询, term是词项查询)

```json
GET blog/_doc/_search
{
    "query": {
        "bool": {
            "must":[
                {
                    "term": {
                        "title": "zhan san"
                    }
                }
            ]
        }
    }
}
```

分词之后, term查询只能按照分词后的结果来查询, **所以我们要选择适合的分词器**

<br>

### search_analyzer

查询时候的分词器

默认情况下, 如果没有配置 search_analyzer, 则查询时则看查询的字段映射中有没有analyzer,如果有,则使用analyzer. 如果没有analyzer,则使用ES默认的分词器.

如果有配置 search_analyzer, 就使用 search_analyzer 来进行分词

**一般情况下, 我们在索引数据时使用定义的 analyzer, 查询的时候也会使用 analyzer, 使保持一致**

特殊情况下才会使用 search_analyzer

<br>

### normalizer

用于解析前(索引或查询之前)的标准化配置.

比如在ES中, 对于我们不想切分的字符串, 我们通常会将其设置为 keyword, 搜索的时候也是使用整个词进行搜索, 如果在索引前没有做好数据清洗, 导致大小写不一致, 例如pythonboy和PYTHONBOY, 此时, 我们就可以使用normalizer在索引之前以及查询之前进行文档的标准化.

> 如果我们在创建索引时没有使用normalizer, 然后添加两个文档, 字段分别是大写和小写, 那么在索引和查询的时候, 会区分大小写, 如果使用normalizer, 就不用区分大小写.

举个例子:

1. 创建索引:

   ```json
   PUT blog
   {   
       "settings": {
           "number_of_replicas": 0,
           "analysis": {
               "normalizer": {
                   "my_normalizer": {
                       "type": "custom", 
                       "filter": ["lowercase"]
                   }
               }
           }
       },
       "mappings": {
           "_doc": {
               "properties": {
                   "title": {
                       "type": "keyword",
                       "normalizer": "my_normalizer"
                   }
               }
           }   
       }
   }
   ```

2. 添加两个文档:

   ```json
   POST blog/_doc/1
   {
      "title": "javaboy"
   }
   ```

   ```json
   POST blog/_doc/2
   {
      "title": "JAVABOY"
   }
   ```

3. 搜索文档: 此时可以检索到大小写两个文档 

   ```json
   GET/POST blog/_doc/_search
   {
       "query": {
           "bool": {
               "must":[
                   {
                       "term": {
                           "title": "JAVABOY"
                       }
                   }
               ]
           }
       }
   }
   ```

<br>

### boost (权重)

boost 参数可以设置字段的权重

boost有两种使用思路, 一种是定义 mappings 的时候使用, 在指定字段类型时使用, 另一种就是在查询中使用.

实际开发中建议在查询中使用:

1. 创建索引: 定义了 title1 和 title2 字段

   ```json
   PUT blog
   {   
       "settings": {
           "number_of_replicas": 0
       },
       "mappings": {
           "_doc": {
               "properties": {
                   "title1": {
                       "type": "text"
                   },
                   "title2": {
                       "type": "text"
                   }
               }
           }   
       }
   }
   ```

2. 插入文档: 

   ```json
   POST blog/_doc/1
   {
     "title1": "春秋系列之齐桓公, 春秋系列之齐桓公", 
     "title2": "春秋系列之齐桓公"
   }
   ```

   ```json
   POST blog/_doc/2
   {
     "title1": "春秋系列之齐桓公", 
     "title2": "春秋系列之齐桓公, 春秋系列之齐桓公"
   }
   ```

3. 根据权重检索文档:

   可以使用 `^` 字符语法为单个字段提升权重，在字段名称的末尾添加 `^boost` ，其中 `boost` 是一个浮点数,

   默认 boost 值为 1

   ```json
   GET/POST blog/_doc/_search
   {
       "query": {
           "bool": {
               "must":[
                   {
                       "multi_match": {
                           "query": "齐桓公",
                           "fields": ["title1^2", "title2"]
                       }
                   }
               ]
           }
       }
   }
   ```

<br>

### coerce

coerce 用来清除脏数据, 默认为 true

例如一个字段是 integer 类型, 在 JSON 中, 用户写错了, 写成了 “666”, 默认情况下添加文档的时候也是没有问题的, 就是 coerce 在起作用:

1. 创建索引:

   ```json
   PUT blog
   {   
       "settings": {
           "number_of_replicas": 0
       },
       "mappings": {
           "_doc": {
               "properties": {
                   "age": {
                       "type": "integer"
                   }
               }
           }   
       }
   }
   ```

2. 添加文档: 

   ```json
   POST blog/_doc/1
   {
      "age": "66"
   }
   ```

如果需要修改 coerce, 方式如下: 在定义类型时, 指定 coerce 为 false, 

修改之后, 数字类型只能接受数字类型的数据了, 不可以是字符串.

1. 创建索引: 

   ```json
   PUT blog
   {   
       "settings": {
           "number_of_replicas": 0
       },
       "mappings": {
           "_doc": {
               "properties": {
                   "age": {
                       "type": "integer",
                       "coerce": false
                   }
               }
           }   
       }
   }
   ```

   或者在全局进行指定: 

   ```json
   {   
       "settings": {
           "number_of_replicas": 0,
           "index.mapping.coerce": false
       },
       "mappings": {
           "_doc": {
               "properties": {
                   "age": {
                       "type": "integer"
                   }
               }
           }   
       }
   }
   ```

2. 再次提交以上数据, 会抛出异常: 

   ```json
   {
       "error": {
           "root_cause": [
               {
                   "type": "mapper_parsing_exception",
                   "reason": "failed to parse field [age] of type [integer]"
               }
           ],
           "type": "mapper_parsing_exception",
           "reason": "failed to parse field [age] of type [integer]",
           "caused_by": {
               "type": "illegal_argument_exception",
               "reason": "Integer value passed as String"
           }
       },
       "status": 400
   }
   ```

<br>

### copy_to

这个属性可以将多个字段的值, 复制到一个字段中, 定义方式如下: 

1. 创建索引: 

   ```json
   PUT blog
   {   
       "settings": {
           "number_of_replicas": 0
       },
       "mappings": {
           "_doc": {
               "properties": {
                   "content1": {
                       "type": "text",
                       "copy_to": "full_content"
                   },
                   "content2": {
                       "type": "text",
                       "copy_to": "full_content"
                   },
                   "full_content": {
                       "type": "text"
                   }
               }
           }   
       }
   }
   ```

2. 添加文档: 

   ```json
   POST blog/_doc/1
   {
      "content1": "春秋",
      "content2": "史记"
   }
   ```

3. 检索文档: 

   ```json
   GET/POST blog/_doc/_search
   {
       "query": {
           "match": {
               "full_content": "春秋"
           }
       }
   }
   ```

   检索文档**看不到 full_content 的内容**, 但是可以使用full_content字段来进行检索.

<br>

### doc_values 和 fielddata

ES 中的搜索只要用到倒排索引, doc_values 参数是为了加快排序和聚合操作而生的, 当建立倒排索引的时候, 会额外增加列式存储映射.

doc_values 默认是开启的, 如果确定某个字段不需要排序, 不需要聚合, 那么可以关闭 doc_values.

fielddata 默认关闭.

大部分的字段在索引时, 都会生成 doc_values(文件后缀, 文件大小等),  除了 text, text字段在查询时, 会生成 fielddata 的数据结构, fielddata在字段首次被聚合, 排序的时候生成.

| doc_values         | fielddata                      |
| ------------------ | ------------------------------ |
| 索引时创建         | 使用时动态创建                 |
| 磁盘               | 内存                           |
| 不占用内存         | 不占用磁盘                     |
| 索引速度稍微低一点 | 文档很多时, 动态创建慢, 占内存 |

doc_values比较常用, fielddata不常用

<br>

### enabled

ES 默认会索引所有的字段, 但是有的字段可能只需要存储, 不需要索引, 此时可以通过 enabled 属性来控制.

只用于mapping中的object字段类型, 当设置为false时，其作用是使ES不去解析该字段，并且该字段**不能被查询和store**，只有在 `_source` 中才能看到（即查询结果中会显示的_source数据）。

enabled 默认为false, 如果设置为true, 就不能根据该字段进行搜索了, 只能展示该字段的数据.(例如图片的exif信息)

<br>

### format

用于日期格式, format可以规范日期格式, 而且一次可以定义多个format.

如果用户没有指定日期的format, 默认的日期格式: `strict_date_optional_time||epoch_mills`, 例外, 所有的日期格式, 可以在ES官网查看详细.

1. 创建索引: 多个日期格式之间用 || 隔开, **注意不要空格**

   ```json
   PUT blog
   {   
       "settings": {
           "number_of_replicas": 0
       },
       "mappings": {
           "_doc": {
               "properties": {
                   "create_time": {
                       "type": "date",
                       "format": "yyyy-MM-dd||yyyy-MM-dd HH:mm:ss"
                   }
               }
           }   
       }
   }
   ```

2. 添加文档: 

   正确的

   ```json
   POST blog/_doc/1
   {
      "create_time": "2021-02-08 20:20:02"
   }
   ```

   抛出异常的

   ```json
   POST blog/_doc/1
   {
      "create_time": "2021-02-08T20:20:02"  //错误的
   }
   ```

<br>

### ignore_above

用于指定分词和索引的字符串最大长度, 超过 ignore_above 长度的话, 该字段将不会被索引, 这个字段只适用于 keyword 类型.

动态映射时, ignore_above 的默认值是256个长度.

1. 创建索引:

   ```json
   PUT blog
   {   
       "settings": {
           "number_of_replicas": 0
       },
       "mappings": {
           "_doc": {
               "properties": {
                   "title": {
                       "type": "keyword",
                       "ignore_above": 10
                   }
               }
           }   
       }
   }
   ```

2. 添加文档: 

   ```json
   POST blog/_doc/1
   {
      "title": "春秋五霸之齐桓公,春秋五霸之齐桓公,"
   }
   ```

3. 搜索文档: 没有被索引, 搜索不到(搜索所有文档时可以)

   ```json
   GET/POST blog/_doc/_search
   {
       "query": {
           "term": {
               "title": "春秋五霸之齐桓公,春秋五霸之齐桓公,"
           }
       }
   }
   ```

<br>

### Ignore_malformed

可以忽略不规则的数据

该参数默认为false, 不忽略, 遇到不规则的数据会抛出异常.

<br>

### include_in_all

这个是针对 _all 字段的, 但是在 ES7.X版本中, 该字段已经被废弃了, 替代品是 copy_to

添加文档时, 会默认将所有的字段和分词词项存储在 _all 字段中, 当使用全文检索时, 会根据 _all 字段进行检索.

默认是true, 如果不希望某字段添加到 _all 中, 可以设置为 false.

**es6.6 不支持了**

```json
{
    "error": {
        "root_cause": [
            {
                "type": "mapper_parsing_exception",
                "reason": "[include_in_all] is not allowed for indices created on or after version 6.0.0 as [_all] is deprecated. As a replacement, you can use an [copy_to] on mapping fields to create your own catch all field."
            }
        ],
        "type": "mapper_parsing_exception",
        "reason": "Failed to parse mapping [_doc]: [include_in_all] is not allowed for indices created on or after version 6.0.0 as [_all] is deprecated. As a replacement, you can use an [copy_to] on mapping fields to create your own catch all field.",
        "caused_by": {
            "type": "mapper_parsing_exception",
            "reason": "[include_in_all] is not allowed for indices created on or after version 6.0.0 as [_all] is deprecated. As a replacement, you can use an [copy_to] on mapping fields to create your own catch all field."
        }
    },
    "status": 400
}
```

 <br>

### index

指定一个字段是否被索引, 该属性为true表示字段被索引, false则不被索引.

默认是true, 被索引, 当设置为false，表明该字段**不能被查询**，如果查询会报错。但是可以被store。当该文档被查出来时，在_source中也会显示出该字段.

<br>

### index_options (了解)

控制索引时, 哪些信息被存储到倒排索引中(用在text字段中), 有四种取值:

| index_options | remark                                         |
| ------------- | ---------------------------------------------- |
| docs          | 只存储文档编号, 默认即此                       |
| freqs         | 在 docs 基础上, 存储词项频率                   |
| positions     | 在 freqs 基础上, 存储词项的偏移位置            |
| offsets       | 在position基础上, 存储词项开始和结束的字符位置 |

<br>

### norms (了解)

对字段评分有用, text字段默认开启 norms, 如果不是特别需要, 不要开启 norms.

<br>

### null_value (了解)

在 ES 中, 值为null的字段不索引也不可以被搜索, null_value可以让值为null的字段显式的被索引和被搜索.

<br>

### position_increment_gap

被解析的text字段会将term的位置考虑进去, 目的是为了支持**近似查询和短语查询**, 当我们去索引含有多个值的text字段时, 会在各个值之间添加一个假想的空间, 将值隔开, 这样就可以有效避免一些无意义的短语匹配, 间隙大小通过 position_increment_gap 来控制, 默认值是 100.

<br>

### properties(常用)

指定有哪些字段

<br>

### similarity(了解)

指定文档的评分模型, 默认有三种: 

| similarity | remark                     |
| ---------- | -------------------------- |
| BM25       | es 和 lucene默认的评分模型 |
| classic    | TF/IDF评分                 |
| boolean    | boolean模型评分            |

<br>

### store(了解)

默认情况下, 字段会被索引, 也可以搜索, 但是不会存储, 虽然不会被储存的, 但是 _source 中有一个字段的备份, 如果想将字段存储下来, 可以通过配置实现.

<br>

### term_vectors(了解)

是通过分词器产生的信息, 包括:

- 一组 terms
- 每个term的位置
- term的首字符/尾字符与原始字符串原点的偏移量

term_vectors 的取值:

| 取值                   | 备注                      |
| ---------------------- | ------------------------- |
| no                     | 不存储信息, 默认即此      |
| yes                    | term被存储                |
| with_positions         | 在yes的基础上增加位置信息 |
| with_offsets           | 在yes的基础上增加偏移量   |
| with_positions_offsets | term, 位置, 偏移量都存储  |



<br><br><br>

###### 完











